{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# This notebook will create the df_timit and save it as a csv for later use. \n",
    "    - We need to step through all the directories in the timit data store and create necessary rows\n",
    "    - Also need to augment the data with some additional info\n",
    "    \n",
    "## Dialect information\n",
    "     dr1:  New England\n",
    "     dr2:  Northern\n",
    "     dr3:  North Midland\n",
    "     dr4:  South Midland\n",
    "     dr5:  Southern\n",
    "     dr6:  New York City\n",
    "     dr7:  Western\n",
    "     dr8:  Army Brat (moved around)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T00:01:18.069972Z",
     "start_time": "2026-01-27T00:01:17.881133Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "#increase some pandas vars\n",
    "pd.set_option('max_colwidth', 100)\n",
    "sys_name = os.uname()[1]\n",
    "\n",
    "# TIMIT 数据集的数据存放路径\n",
    "def data_path_train():    \n",
    "    if sys_name == 'Hive':\n",
    "        return \"/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_true\"\n",
    "        #return \"/home/logan/drive/Research/Data_Stores/Timit/Data/timit/TIMIT/TRAIN\"\n",
    "    else:\n",
    "        # return \"/home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_true\"\n",
    "        # return \"/home/exx/shudong/project/vocal_tract_reconstruction/dataset/TIMIT\"\n",
    "        # return \"/home/exx/shudong/project/vocal_tract_reconstruction/dataset/generated_TIMIT\"\n",
    "        return \"/home/exx/shudong/project/vocal_tract_reconstruction/dataset/extracted_smoothed_crossSpeaker\"\n",
    "        # return \"/home/exx/shudong/project/vocal_tract_reconstruction/dataset/alignment_data_fake\"\n",
    "        # return \"/home/exx/shudong/project/vocal_tract_reconstruction/dataset/ASVspoof2021_DF_eval/mfa_output_DF_TTS/mfa_input/wav\"\n",
    "        # return \"/home/exx/shudong/project/vocal_tract_reconstruction/dataset/ASVspoof2021_DF_eval/mfa_output_DF_VC/mfa_input/wav\"\n",
    "        # return \"/home/exx/shudong/project/vocal_tract_reconstruction/dataset/ASVspoof2021_DF_eval/mfa_output_DF_hybrid/mfa_input/wav\"\n",
    "        \n",
    "def data_path_lyre_true_extend(): \n",
    "    return \"/home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird_comp/real/\"\n",
    "\n",
    "def data_path_lyre_fake_extend(): \n",
    "    return \"/home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird_comp/fake/\"\n",
    "    \n",
    "def data_path_fakes():\n",
    "    if sys_name == 'Hive':\n",
    "        return \"/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_fakes/\"\n",
    "    else:\n",
    "        # return \"//home/logan/SynologyDrive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_fakes/\"\n",
    "        # return \"/home/exx/shudong/project/vocal_tract_reconstruction/dataset/ASVspoof2021_DF_eval/mfa_output_DF_TTS/mfa_input/wav/\"\n",
    "        return \"/home/exx/shudong/project/vocal_tract_reconstruction/dataset/ASVspoof2021_DF_eval/mfa_output_DF_hybrid/mfa_input/wav/\"\n",
    "\n",
    "def data_path_test():\n",
    "    sys_name = os.uname()[1]\n",
    "    if sys_name == 'Hive':\n",
    "        return \"/home/logan/drive/Research/Data_Stores/Timit/Data/timit/TIMIT/TEST\"\n",
    "    else:\n",
    "        return \"/home/logan/SynologyDrive/Research/Data_Stores/Timit/Data/timit/TIMIT/TEST\"\n",
    "\n",
    "def append_file(root, file, hive=False, add_slash=False):\n",
    "    if hive:\n",
    "        mod_root = \"E:\\\\SynologyDrive\\\\\" + root[root.find('Research'):]\n",
    "        if add_slash:\n",
    "            output = mod_root  + '\\\\' + file\n",
    "        else:\n",
    "            output = mod_root  + file\n",
    "        output =  output.replace('/', '\\\\')\n",
    "    else:\n",
    "        if add_slash:\n",
    "            output = root + '/' + file\n",
    "        else:\n",
    "            output = root + file\n",
    "    return output\n",
    "\n",
    "def cut_speaker_id(path):\n",
    "    return path[path.rfind('/') + 1:]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T00:01:23.235727Z",
     "start_time": "2026-01-27T00:01:23.227333Z"
    }
   },
   "source": [
    "# arpabet to ipa conversion\n",
    "arpa_key = ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'axr', 'ay', 'eh', 'er', 'ey', 'ih', 'ix', 'iy', \\\n",
    "                'ow', 'oy', 'uh', 'uw', 'ux' , 'b', 'ch', 'd', 'dh', 'dx', 'el', 'em', 'en', 'f', \\\n",
    "                'g', 'h', 'hh', 'jh', 'k', 'l', 'm', 'n', 'ng', 'nx', 'p', 'q', 'r', 's', 'sh', \\\n",
    "                't', 'th', 'v', 'w', 'wh', 'y', 'z', 'zh', 'ax-h', 'bcl', 'dcl', 'eng', 'gcl', 'hv', \\\n",
    "                'kcl', 'pcl', 'tcl', 'pau', 'epi', 'h#', 'spn']\n",
    "ipa_key = ['ɑ', 'æ', 'ʌ', 'ɔ', 'aʊ', 'ə', 'ɚ', 'aɪ', 'ɛ', 'ɝ', 'eɪ', 'ɪ', 'ɨ', 'i', 'oʊ', 'ɔɪ', \\\n",
    "                'ʊ', 'u', 'ʉ', 'b', 'tʃ', 'd', 'ð', 'ɾ', 'l̩', 'm̩', 'n̩', 'f', 'ɡ', 'h', 'h', 'dʒ', 'k', \\\n",
    "                'l', 'm', 'n', 'ŋ', 'ɾ̃', 'p', 'ʔ', 'ɹ', 's', 'ʃ', 't', 'θ', 'v', 'w', 'ʍ', 'j', 'z', \\\n",
    "                'ʒ', 'ə̥', 'b̚', 'd̚', 'ŋ̍', 'ɡ̚', 'ɦ', 'k̚', 'p̚', 't̚', 'N/A', 'N/A', 'N/A', 'N/A']\n",
    "\n",
    "ipa_conversion = dict(zip(arpa_key, ipa_key))\n",
    "def convert_to_ipa(arpa_key):\n",
    "    output = []\n",
    "    for key in arpa_key:\n",
    "        output.append(ipa_conversion[key])\n",
    "    return output\n",
    "\n",
    "arpa_conversion = dict(zip(ipa_key, arpa_key))\n",
    "def convert_from_ipa(ipa_key):\n",
    "    output = []\n",
    "    for key in ipa_key:\n",
    "        output.append(arpa_conversion.get(key, 'N/A'))\n",
    "    return output\n",
    "\n",
    "def join_word_phoneme(df_wrd, df_phn, audio_file):\n",
    "    new_df = df_wrd.merge(df_phn, 'outer', on=('sample_id', 'speaker_id'), \\\n",
    "                          suffixes=('_word', '_phoneme'))\n",
    "    new_df = new_df[(new_df.start_phoneme >= new_df.start_word) &\n",
    "                    (new_df.end_phoneme <= new_df.end_word)]\n",
    "    try:\n",
    "        new_df['ipa'] = convert_to_ipa(new_df['arpabet'])\n",
    "        new_df['filepath'] = audio_file\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in file {audio_file}: {e}\")\n",
    "        print(f\"Arpabet values: {new_df['arpabet'].unique()}\")\n",
    "        print(f\"DataFrame shape: {new_df.shape}\")\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    if audio_file[audio_file.rfind('/') - 5] == 'M':\n",
    "        new_df['sex'] = 'm'\n",
    "    else:\n",
    "        new_df['sex'] = 'f'\n",
    "    return new_df\n",
    "\n",
    "def ipa_join_word_phoneme(df_wrd, df_phn, audio_file):\n",
    "    new_df = df_wrd.merge(df_phn, 'outer', on=('sample_id', 'speaker_id'), \\\n",
    "                          suffixes=('_word', '_phoneme'))\n",
    "    new_df = new_df[(new_df.start_phoneme >= new_df.start_word) &\n",
    "                    (new_df.end_phoneme <= new_df.end_word)]\n",
    "    try:\n",
    "        new_df['arpabet'] = convert_from_ipa(new_df['ipa'])\n",
    "        new_df['filepath'] = audio_file\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    if audio_file[audio_file.rfind('/') - 5] == 'M':\n",
    "        new_df['sex'] = 'm'\n",
    "    else:\n",
    "        new_df['sex'] = 'f'\n",
    "    return new_df"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timit master creation\n",
    "\n",
    "### Training\n",
    "Date 06/05/2025 Training已校对\n",
    "TIMIT数据集有.WAV和.WAV.wav两种格式文件，只有.wav文件为原始音频文件\n",
    "\n",
    "    原TIMIT数据集为ARPABET音标集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T02:16:41.438356Z",
     "start_time": "2026-01-27T02:16:41.209527Z"
    }
   },
   "source": [
    "#step through all directories in data path, consume all files with s^Y vimame name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_train = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', 'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "for root, dirs, files in list(os.walk(data_path_train())):\n",
    "    for file in tqdm(files, position=0, leave=True):\n",
    "        if file.endswith('.wav'):\n",
    "        # if file.endswith('.WAV'):\n",
    "            sample_id = file.split('.')[0]\n",
    "            speaker_id = cut_speaker_id(root) \n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, sample_id+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'arpabet'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            # df_train = pd.concat(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "            #                         append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "            new_df = join_word_phoneme(df_wrd, df_phn, \n",
    "                            append_file(root, file, hive=False, add_slash=True))\n",
    "            df_train = pd.concat([df_train, new_df], ignore_index=True)\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_train['index_phoneme'] = -1\n",
    "grouped_df = df_train.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_train.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_train = df_train[df_train.ipa.isin(all_ph)]\n",
    "print(len(df_train))\n",
    "\n",
    "#save dataframe\n",
    "# df_train.to_csv('../metadata/timit.csv', index=False)\n",
    "# df_train.to_csv('../metadata/timit_generated.csv', index=False)\n",
    "df_train.to_csv('../metadata/test_crossSpeaker.csv', index=False)\n",
    "print('Done')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 765.63it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1053.18it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1084.08it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1099.35it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1094.90it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1022.19it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1054.84it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1064.75it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1008.06it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 989.57it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 965.15it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 981.64it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 916.64it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1081.22it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1112.55it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1077.33it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1027.57it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1030.16it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 988.17it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1086.96it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1072.99it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1067.93it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 960.12it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1018.22it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1032.51it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1087.03it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 990.68it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1004.02it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1089.29it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1090.21it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1031.75it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1021.63it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1061.38it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1104.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1270\n",
      "Done\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ASVspoof2021 DF数据子集处理"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T03:23:59.672956Z",
     "start_time": "2025-06-11T03:23:54.531842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#step through all directories in data path, consume all files with s^Y vimame name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_tts = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', \\\n",
    "             'sex', 'filepath'])\n",
    "for root, dirs, files in list(os.walk(data_path_fakes())):     #training data\n",
    "    for file in tqdm(files, position=0, leave=True):\n",
    "        if file.endswith('.wav'):\n",
    "            sample_id = file.split('.')[0]\n",
    "            speaker_id = cut_speaker_id(root) \n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start_word', 'end_word', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            fpath = append_file(root, file, hive=False,add_slash=True)\n",
    "            df_wrd['filepath'] = fpath\n",
    "            if fpath[fpath[:fpath.rfind('_')].rfind('_') - 5] == 'M':\n",
    "                df_wrd['sex'] = 'm'\n",
    "            else:\n",
    "                df_wrd['sex'] = 'f'\n",
    "            df_tts = pd.concat([df_tts, df_wrd], ignore_index=True)\n",
    "\n",
    "print(len(df_tts))\n",
    "\n",
    "#save dataframe\n",
    "df_tts.to_csv('../metadata/df_tts.csv', index=False)\n",
    "print('Done')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 1622.26it/s]\n",
      "100%|██████████| 112/112 [00:00<00:00, 1675.23it/s]\n",
      "100%|██████████| 144/144 [00:00<00:00, 1669.19it/s]\n",
      "100%|██████████| 108/108 [00:00<00:00, 1660.64it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 1636.56it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 1631.05it/s]\n",
      "100%|██████████| 336/336 [00:00<00:00, 1616.63it/s]\n",
      "100%|██████████| 336/336 [00:00<00:00, 1648.74it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 1032.90it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1287.11it/s]\n",
      "100%|██████████| 208/208 [00:00<00:00, 1593.08it/s]\n",
      "100%|██████████| 104/104 [00:00<00:00, 1581.91it/s]\n",
      "100%|██████████| 136/136 [00:00<00:00, 1585.40it/s]\n",
      "100%|██████████| 128/128 [00:00<00:00, 1590.61it/s]\n",
      "100%|██████████| 252/252 [00:00<00:00, 1602.99it/s]\n",
      "100%|██████████| 340/340 [00:00<00:00, 1592.68it/s]\n",
      "100%|██████████| 312/312 [00:00<00:00, 1563.86it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 1546.56it/s]\n",
      "100%|██████████| 268/268 [00:00<00:00, 1538.50it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 1287.90it/s]\n",
      "100%|██████████| 132/132 [00:00<00:00, 1526.78it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 1490.26it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 1508.39it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 1507.60it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1511.13it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 1534.90it/s]\n",
      "100%|██████████| 144/144 [00:00<00:00, 1552.79it/s]\n",
      "100%|██████████| 244/244 [00:00<00:00, 1517.11it/s]\n",
      "100%|██████████| 380/380 [00:00<00:00, 1756.63it/s]\n",
      "100%|██████████| 296/296 [00:00<00:00, 1869.23it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 1830.66it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1910.65it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1823.66it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 1837.71it/s]\n",
      "100%|██████████| 112/112 [00:00<00:00, 1972.20it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 1874.28it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 1776.79it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 1891.70it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 1885.93it/s]\n",
      "100%|██████████| 112/112 [00:00<00:00, 1903.43it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 1775.55it/s]\n",
      "100%|██████████| 140/140 [00:00<00:00, 1903.14it/s]\n",
      "100%|██████████| 288/288 [00:00<00:00, 1817.18it/s]\n",
      "100%|██████████| 264/264 [00:00<00:00, 1891.47it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 1827.32it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 1775.45it/s]\n",
      "100%|██████████| 136/136 [00:00<00:00, 1799.18it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 1741.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14949\n",
      "Done\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fake master creation\n",
    "* First cell is for fully labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This second cell is for word labels only"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T03:03:18.660567Z",
     "start_time": "2025-06-11T03:03:18.538960Z"
    }
   },
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_timit = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', \\\n",
    "             'sex', 'filepath'])\n",
    "\n",
    "for root, _, files in os.walk(data_path_fakes()):     #deep fakes data\n",
    "    for file in files:        \n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            speaker_id = file[16:21]\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD')\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            fpath = append_file(root, file, hive=True)\n",
    "            df_wrd['filepath'] = fpath\n",
    "            if fpath[fpath[:fpath.rfind('_')].rfind('_') - 5] == 'M':\n",
    "                df_wrd['sex'] = 'm'\n",
    "            else:\n",
    "                df_wrd['sex'] = 'f'\n",
    "            df_timit = df_timit.append(df_wrd, ignore_index=True)\n",
    "print(\"Len: \", df_timit.count()[0])\n",
    "\n",
    "df_timit.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_timit.to_csv('../../data/real_time_master.csv', index=False)"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/exx/shudong/project/vocal_tract_reconstruction/dataset/ASVspoof2021_DF_eval/mfa_output_DF_TTS/mfa_input/wav/p362DF_E_3999875.WRD'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 15\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m#get word info\u001B[39;00m\n\u001B[1;32m     14\u001B[0m wrd_file \u001B[38;5;241m=\u001B[39m append_file(root, sample_id\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.WRD\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 15\u001B[0m df_wrd \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrd_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelimiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstart\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mword\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m df_wrd[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msample_id\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m sample_id\n\u001B[1;32m     17\u001B[0m df_wrd[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspeaker_id\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m speaker_id\n",
      "File \u001B[0;32m~/anaconda3/envs/py310_vocal_tract_reconstruction/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/py310_vocal_tract_reconstruction/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/anaconda3/envs/py310_vocal_tract_reconstruction/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/py310_vocal_tract_reconstruction/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/anaconda3/envs/py310_vocal_tract_reconstruction/lib/python3.10/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/exx/shudong/project/vocal_tract_reconstruction/dataset/ASVspoof2021_DF_eval/mfa_output_DF_TTS/mfa_input/wav/p362DF_E_3999875.WRD'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gentle"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_word_x</th>\n",
       "      <th>end_word_x</th>\n",
       "      <th>word_x</th>\n",
       "      <th>sample_id_x</th>\n",
       "      <th>start_phoneme_x</th>\n",
       "      <th>end_phoneme_x</th>\n",
       "      <th>sex_x</th>\n",
       "      <th>arpabet_x</th>\n",
       "      <th>ipa_x</th>\n",
       "      <th>filepath_x</th>\n",
       "      <th>...</th>\n",
       "      <th>word_y</th>\n",
       "      <th>sample_id_y</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>start_phoneme_y</th>\n",
       "      <th>end_phoneme_y</th>\n",
       "      <th>sex_y</th>\n",
       "      <th>arpabet_y</th>\n",
       "      <th>ipa_y</th>\n",
       "      <th>filepath_y</th>\n",
       "      <th>index_phoneme_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [start_word_x, end_word_x, word_x, sample_id_x, start_phoneme_x, end_phoneme_x, sex_x, arpabet_x, ipa_x, filepath_x, index_phoneme_x, start_word_y, end_word_y, word_y, sample_id_y, speaker_id, start_phoneme_y, end_phoneme_y, sex_y, arpabet_y, ipa_y, filepath_y, index_phoneme_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8,
   "source": "df_test.merge(df_train, left_on='speaker_id', right_on='speaker_id')\n"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_test = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/guesswho_fakes'\n",
    "for root, dirs, files in tqdm(list(os.walk(data_path)), position=0, leave=True):     #training data\n",
    "    for file in files:\n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            speaker_id = file[16:21]            \n",
    "                \n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, sample_id+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'ipa'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            df_test = df_test.append(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "\n",
    "\n",
    "df_raw = df_test\n",
    "#add phoneme indices for bigram analysis\n",
    "df_test['index_phoneme'] = -1\n",
    "grouped_df = df_test.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_test.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_test = df_test[df_test.ipa.isin(all_ph)]\n",
    "df_test.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_test.to_csv('../../data/real_time_gentle_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('../../data/real_time_gentle_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_word</th>\n",
       "      <th>end_word</th>\n",
       "      <th>word</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>start_phoneme</th>\n",
       "      <th>end_phoneme</th>\n",
       "      <th>sex</th>\n",
       "      <th>arpabet</th>\n",
       "      <th>ipa</th>\n",
       "      <th>filepath</th>\n",
       "      <th>index_phoneme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [start_word, end_word, word, sample_id, speaker_id, start_phoneme, end_phoneme, sex, arpabet, ipa, filepath, index_phoneme]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrebird Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (748468952.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"/tmp/ipykernel_7156/748468952.py\"\u001B[0;36m, line \u001B[0;32m8\u001B[0m\n\u001B[0;31m    for root, dirs, files in tqdm(list(os.walk(data_path_test)):     #training data\u001B[0m\n\u001B[0m                                                               ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_test = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird/true/'\n",
    "for root, dirs, files in tqdm(list(os.walk(data_path_test)):     #training data\n",
    "    for file in files:\n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            if 'trump' in file:\n",
    "                speaker_id = 'trump'\n",
    "            else:\n",
    "                speaker_id = 'obama'\n",
    "                \n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, sample_id+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'arpabet'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            df_test = df_test.append(join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_test['index_phoneme'] = -1\n",
    "grouped_df = df_test.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_test.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_test = df_test[df_test.ipa.isin(all_ph)]\n",
    "df_test.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_timit.to_csv('../../data/lyre_bird_true_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_timit = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', \\\n",
    "             'sex', 'filepath'])\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird/real/'\n",
    "for root, _, files in tqdm(list(os.walk(data_path))):     #deep fakes data\n",
    "    for file in files:        \n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            if 'trump' in file:\n",
    "                speaker_id = 'trump'\n",
    "            else:\n",
    "                speaker_id = 'obama'\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start_word', 'end_word', 'word'])            \n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            fpath = append_file(root, file, hive=True, add_slash=True)\n",
    "            df_wrd['filepath'] = fpath\n",
    "            if fpath[fpath[:fpath.rfind('_')].rfind('_') - 5] == 'M':\n",
    "                df_wrd['sex'] = 'm'\n",
    "            else:\n",
    "                df_wrd['sex'] = 'f'\n",
    "            df_timit = df_timit.append(df_wrd, ignore_index=True)\n",
    "print(\"Len: \", df_timit.count()[0])\n",
    "\n",
    "df_timit.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_timit.to_csv('../../data/lyre_bird_true_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_timit = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', \\\n",
    "             'sex', 'filepath'])\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird/fake/'\n",
    "for root, _, files in tqdm(list(os.walk(data_path))):     #deep fakes data\n",
    "    for file in files:        \n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            if 'trump' in file:\n",
    "                speaker_id = 'trump'\n",
    "            else:\n",
    "                speaker_id = 'obama'\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start_word', 'end_word', 'word'])            \n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            fpath = append_file(root, file, hive=True, add_slash=True)\n",
    "            df_wrd['filepath'] = fpath\n",
    "            if fpath[fpath[:fpath.rfind('_')].rfind('_') - 5] == 'M':\n",
    "                df_wrd['sex'] = 'm'\n",
    "            else:\n",
    "                df_wrd['sex'] = 'f'\n",
    "            df_timit = df_timit.append(df_wrd, ignore_index=True)\n",
    "print(\"Len: \", df_timit.count()[0])\n",
    "\n",
    "df_timit.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_timit.to_csv('../../data/lyre_bird_fake_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASV Spoof\n",
    "\n",
    "## True set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_test = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/ASV_spoof/real/'\n",
    "for root, dirs, files in tqdm(list(os.walk(data_path)), position=0, leave=True):     #training data\n",
    "    for file in files:\n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            speaker_id = root[-16:-9]            \n",
    "                \n",
    "            #get phoneme infor\n",
    "            phn_file = append_file(root, sample_id+'.PHN', add_slash=True)\n",
    "            df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'ipa'])\n",
    "            df_phn['sample_id'] = sample_id\n",
    "            df_phn['speaker_id'] = speaker_id\n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            df_test = df_test.append(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_test['index_phoneme'] = -1\n",
    "grouped_df = df_test.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_test.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_test = df_test[df_test.ipa.isin(all_ph)]\n",
    "df_test.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_test.to_csv('../../data/asv_spoof_true_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_test = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', 'start_phoneme', \n",
    "        'end_phoneme', 'sex', 'arpabet', 'ipa', 'filepath'])\n",
    "\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/ASV_spoof/fake/'\n",
    "for root, dirs, files in tqdm(list(os.walk(data_path)), position=0, leave=True):     #training data\n",
    "    for file in files:\n",
    "        if 'wav' in file.lower():\n",
    "            try:\n",
    "                sample_id = file[:-4]\n",
    "                speaker_id = root[-16:-9]            \n",
    "\n",
    "                #get phoneme infor\n",
    "                phn_file = append_file(root, sample_id+'.PHN', add_slash=True)\n",
    "                df_phn = pd.read_csv(phn_file, delimiter=' ', names=['start', 'end', 'ipa'])\n",
    "                df_phn['sample_id'] = sample_id\n",
    "                df_phn['speaker_id'] = speaker_id\n",
    "\n",
    "                #get word info\n",
    "                wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "                df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start', 'end', 'word'])\n",
    "                df_wrd['sample_id'] = sample_id\n",
    "                df_wrd['speaker_id'] = speaker_id\n",
    "                df_test = df_test.append(ipa_join_word_phoneme(df_wrd, df_phn, \n",
    "                                    append_file(root, file, hive=True, add_slash=True)), ignore_index=True)\n",
    "            except:\n",
    "                #likely that wrd or phn file not created, just skip\n",
    "                pass\n",
    "\n",
    "#add phoneme indices for bigram analysis\n",
    "df_test['index_phoneme'] = -1\n",
    "grouped_df = df_test.groupby(['speaker_id', 'sample_id', 'start_word'])\n",
    "for key, item in grouped_df:\n",
    "    new_indices = list(range(len(item.index_phoneme)))\n",
    "    df_test.loc[item.index, 'index_phoneme'] = new_indices\n",
    "    \n",
    "#filter out unknown ipa values (epi, silence)\n",
    "all_ph = ['ʃ', 'ɨ', 'ɦ', 'ɛ', 'd̚', 'dʒ', 'ɪ', 'd', 'ʌ', 'k̚', 'k', 's', 'ʉ',\n",
    "       'ʔ', 'n̩', 'ɡ̚', 'ɡ', 'ɹ', 'w', 'ɔ', 'ɾ', 'ɚ', 'l', 'j', 'ʊ', 'n',\n",
    "       'æ', 'm', 'ɔɪ', 'ə', 'ð', 't̚', 'i', 'v', 'f', 't', 'p̚', 'oʊ',\n",
    "       'h', 'tʃ', 'b̚', 'b', 'ɑ', 'm̩', 'ŋ', 'aɪ', 'θ', 'ə̥', 'eɪ',\n",
    "       'p', 'aʊ', 'ɝ', 'ɾ̃', 'z', 'l̩', 'u', 'ʒ', 'ŋ̍']\n",
    "\n",
    "df_test = df_test[df_test.ipa.isin(all_ph)]\n",
    "df_test.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_test.to_csv('../../data/asv_spoof_gentle_master.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"============Old==============================\"\"\"\n",
    "#step through all directories in data path, consume all files with same name (different extensions)\n",
    "#to populate a single row\n",
    "tmp = 0\n",
    "df_timit = pd.DataFrame(columns=['start_word', 'end_word', 'word', 'sample_id', 'speaker_id', \\\n",
    "             'sex', 'filepath'])\n",
    "data_path = '/home/logan/drive/Research/guesswho_new/guesswho18/data/deepfake_data/lyre_bird/fake/'\n",
    "for root, _, files in tqdm(list(os.walk(data_path))):     #deep fakes data\n",
    "    for file in files:        \n",
    "        if 'wav' in file.lower():\n",
    "            sample_id = file[:-4]\n",
    "            speaker_id = root[-16:-9] \n",
    "            \n",
    "            #get word info\n",
    "            wrd_file = append_file(root, sample_id+'.WRD', add_slash=True)\n",
    "            df_wrd = pd.read_csv(wrd_file, delimiter=' ', names=['start_word', 'end_word', 'word'])            \n",
    "            df_wrd['sample_id'] = sample_id\n",
    "            df_wrd['speaker_id'] = speaker_id\n",
    "            fpath = append_file(root, file, hive=True, add_slash=True)\n",
    "            df_wrd['filepath'] = fpath\n",
    "            df_wrd['sex'] = 'f'\n",
    "\n",
    "            df_timit = df_timit.append(df_wrd, ignore_index=True)\n",
    "print(\"Len: \", df_timit.count()[0])\n",
    "\n",
    "df_timit.count()[0]\n",
    "\n",
    "#save dataframe\n",
    "df_timit.to_csv('../../data/asv_spoof_fake_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create consistency validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_dir = '/home/logan/drive/Research/guesswho18/'     #hive\n",
    "#gw_dir = '/User/logan/Gogle_Drive/Research/guesswho18/'     #iMac\n",
    "\n",
    "df_timit_read = pd.read_csv(gw_dir + 'data/timit_master.csv', sep=',',\n",
    "       dtype = {\n",
    "           'start_word' : np.int,\n",
    "           'end_word': np.int,\n",
    "           'word': np.str,\n",
    "           'sample_id': np.str,\n",
    "           'speaker_id': np.str,\n",
    "           'start_phoneme': np.int,\n",
    "           'end_phoneme': np.int,\n",
    "           'arpabet': np.str,\n",
    "           'ipa': str,\n",
    "           'filename': np.str,\n",
    "           'index_phoneme': np.int\n",
    "       })\n",
    "print(\"Timit done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(df_timit_read['filepath'].unique())\n",
    "import random\n",
    "random.seed(13)\n",
    "sampled_paths = random.sample(paths, 490)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_timit_read[df_timit_read.filepath.isin(sampled_paths)]\n",
    "df_sample = df_sample.reset_index()\n",
    "df_sample.drop(columns=['index'])\n",
    "df_sample.to_csv('../../data/consistent_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
